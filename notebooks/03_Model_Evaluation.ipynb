{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcc8 Evaluaci\u00f3n del Modelo Final (M\u00e9tricas y Gr\u00e1ficos)\n",
    "\n",
    "## \ud83c\udfaf Objetivo\n",
    "En este notebook evaluamos el rendimiento del modelo seleccionado (`best_pipeline.pkl`) utilizando datos no vistos (o un subconjunto de validaci\u00f3n).\n",
    "Analizamos m\u00e9tricas clave para clasificaci\u00f3n binaria en el contexto m\u00e9dico.\n",
    "\n",
    "## \ud83d\udcca M\u00e9tricas Principales\n",
    "- **Matriz de Confusi\u00f3n**: \u00bfCu\u00e1ntos enfermos detectamos correctamente (TP) y cu\u00e1ntos sanos alarmamos falsamente (FP)?\n",
    "- **Recall (Sensibilidad)**: Capacidad del modelo para identificar positivos. Es nuestra prioridad.\n",
    "- **Precision**: De los que el modelo dice que est\u00e1n enfermos, \u00bfcu\u00e1ntos lo est\u00e1n realmente?\n",
    "- **F1-Score**: Balance arm\u00f3nico entre Precision y Recall.\n",
    "- **AUC-ROC**: Capacidad discriminante global del modelo.\n",
    "\n",
    "## \ud83d\udd0d Interpretabilidad\n",
    "- **Feature Importance**: \u00bfQu\u00e9 biomarcadores (Edad, Glucosa, Presi\u00f3n) influyen m\u00e1s en la predicci\u00f3n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pycaret.classification import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "MODEL_PATH = \"../models/best_pipeline\"\n",
    "DATA_PATH = \"../data/02_intermediate/process_data.parquet\"\n",
    "CONFIG_PATH = \"../models/model_config.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga del Modelo y Datos de Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. LOAD MODEL & DATA\n",
    "# ==========================================\n",
    "pipeline = load_model(MODEL_PATH)\n",
    "\n",
    "# Load Data for Evaluation\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Use a sample for quick evaluation if dataset is huge\n",
    "df_eval = df.sample(frac=0.2, random_state=123) \n",
    "print(f\"Evaluation Data Shape: {df_eval.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generaci\u00f3n de Predicciones\n",
    "Aplicamos el modelo sobre el set de evaluaci\u00f3n para obtener etiquetas y probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. GENERATE PREDICTIONS\n",
    "# ==========================================\n",
    "predictions = predict_model(pipeline, data=df_eval)\n",
    "# PyCaret appends 'prediction_label' and 'prediction_score'\n",
    "print(predictions.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An\u00e1lisis de Errores (Matriz de Confusi\u00f3n)\n",
    "Visualizamos la distribuci\u00f3n de aciertos y fallos. Nos interesa minimizar los Falsos Negativos (pacientes enfermos diagnosticados como sanos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. CONFUSION MATRIX\n",
    "# ==========================================\n",
    "y_true = predictions[config['target']]\n",
    "y_pred = predictions['prediction_label']\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. M\u00e9tricas de Desempe\u00f1o\n",
    "- **Recall**: Cr\u00edtico para tamizaje m\u00e9dico.\n",
    "- **AUC**: Medida de separabilidad entre clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. METRICS REPORT\n",
    "# ==========================================\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "try:\n",
    "    # Calculate AUC if scores are available\n",
    "    # prediction_score is the probability of the predicted class. \n",
    "    # For AUC we need probability of the positive class.\n",
    "    # PyCaret's prediction_score is max(prob_0, prob_1). \n",
    "    # We assume binary classification 0/1.\n",
    "    # If label is 1, prob_1 = score. If label is 0, prob_1 = 1 - score.\n",
    "    \n",
    "    probs = predictions.apply(lambda x: x['prediction_score'] if x['prediction_label'] == 1 else 1 - x['prediction_score'], axis=1)\n",
    "    auc = roc_auc_score(y_true, probs)\n",
    "    print(f\"ROC AUC Score: {auc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"AUC Calculation Warning: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explicabilidad del Modelo\n",
    "Identificamos los factores de riesgo m\u00e1s importantes seg\u00fan el modelo aprendido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. FEATURE IMPORTANCE\n",
    "# ==========================================\n",
    "# Attempt to plot feature importance using PyCaret or extraction\n",
    "try:\n",
    "    plot_model(pipeline, plot='feature')\n",
    "except:\n",
    "    print(\"PyCaret plot_model failed or not supported for this pipeline. Attempting manual plot.\")\n",
    "    try:\n",
    "        # Extract model from pipeline (usually the last step)\n",
    "        model_step = pipeline.steps[-1][1]\n",
    "        if hasattr(model_step, 'feature_importances_'):\n",
    "            importances = pd.Series(model_step.feature_importances_, index=config['features'])\n",
    "            importances.nlargest(10).plot(kind='barh', title='Top 10 Feature Importances')\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Manual feature importance plot failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}