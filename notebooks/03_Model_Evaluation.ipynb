{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from pycaret.classification import *\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay\n",
                "import os\n",
                "import json\n",
                "\n",
                "# ==========================================\n",
                "# CONFIGURATION\n",
                "# ==========================================\n",
                "MODEL_PATH = \"../models/best_pipeline\"\n",
                "DATA_PATH = \"../data/02_intermediate/process_data.parquet\"\n",
                "CONFIG_PATH = \"../models/model_config.json\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# 1. LOAD MODEL & DATA\n",
                "# ==========================================\n",
                "pipeline = load_model(MODEL_PATH)\n",
                "\n",
                "# Load Data for Evaluation\n",
                "df = pd.read_parquet(DATA_PATH)\n",
                "\n",
                "with open(CONFIG_PATH, 'r') as f:\n",
                "    config = json.load(f)\n",
                "\n",
                "# Use a sample for quick evaluation if dataset is huge\n",
                "df_eval = df.sample(frac=0.2, random_state=123) \n",
                "print(f\"Evaluation Data Shape: {df_eval.shape}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# 2. GENERATE PREDICTIONS\n",
                "# ==========================================\n",
                "predictions = predict_model(pipeline, data=df_eval)\n",
                "# PyCaret appends 'prediction_label' and 'prediction_score'\n",
                "print(predictions.head())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# 3. CONFUSION MATRIX\n",
                "# ==========================================\n",
                "y_true = predictions[config['target']]\n",
                "y_pred = predictions['prediction_label']\n",
                "\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
                "disp.plot(cmap='Blues')\n",
                "plt.title(\"Confusion Matrix\")\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# 4. METRICS REPORT\n",
                "# ==========================================\n",
                "print(\"Classification Report:\")\n",
                "print(classification_report(y_true, y_pred))\n",
                "\n",
                "try:\n",
                "    # Calculate AUC if scores are available\n",
                "    # prediction_score is the probability of the predicted class. \n",
                "    # For AUC we need probability of the positive class.\n",
                "    # PyCaret's prediction_score is max(prob_0, prob_1). \n",
                "    # We assume binary classification 0/1.\n",
                "    # If label is 1, prob_1 = score. If label is 0, prob_1 = 1 - score.\n",
                "    \n",
                "    probs = predictions.apply(lambda x: x['prediction_score'] if x['prediction_label'] == 1 else 1 - x['prediction_score'], axis=1)\n",
                "    auc = roc_auc_score(y_true, probs)\n",
                "    print(f\"ROC AUC Score: {auc:.4f}\")\n",
                "except Exception as e:\n",
                "    print(f\"AUC Calculation Warning: {e}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# 5. FEATURE IMPORTANCE\n",
                "# ==========================================\n",
                "# Attempt to plot feature importance using PyCaret or extraction\n",
                "try:\n",
                "    plot_model(pipeline, plot='feature')\n",
                "except:\n",
                "    print(\"PyCaret plot_model failed or not supported for this pipeline. Attempting manual plot.\")\n",
                "    try:\n",
                "        # Extract model from pipeline (usually the last step)\n",
                "        model_step = pipeline.steps[-1][1]\n",
                "        if hasattr(model_step, 'feature_importances_'):\n",
                "            importances = pd.Series(model_step.feature_importances_, index=config['features'])\n",
                "            importances.nlargest(10).plot(kind='barh', title='Top 10 Feature Importances')\n",
                "            plt.show()\n",
                "    except Exception as e:\n",
                "        print(f\"Manual feature importance plot failed: {e}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}