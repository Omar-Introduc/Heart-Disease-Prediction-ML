{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83e\udd16 Entrenamiento del Modelo Predictivo (PyCaret)\n",
    "\n",
    "## \ud83c\udfaf Objetivo\n",
    "Este notebook orquesta el pipeline de entrenamiento de Machine Learning utilizando **PyCaret**.\n",
    "El objetivo es encontrar y optimizar el mejor algoritmo capaz de predecir la probabilidad de **Enfermedad Card\u00edaca** bas\u00e1ndose en biomarcadores cl\u00ednicos.\n",
    "\n",
    "## \u2699\ufe0f Estrategia de Modelado\n",
    "1. **Preprocesamiento Robusto**: Normalizaci\u00f3n y manejo de outliers.\n",
    "2. **Balanceo de Clases**: Uso de t\u00e9cnicas (SMOTE) para mitigar el desbalance entre pacientes sanos y enfermos.\n",
    "3. **Optimizaci\u00f3n de Recall**: Priorizamos la **Sensibilidad (Recall)** sobre la Precisi\u00f3n.\n",
    "   - *Contexto M\u00e9dico*: Es peor no detectar a un enfermo (Falso Negativo) que alarmar a un sano (Falso Positivo).\n",
    "4. **Selecci\u00f3n de Modelos**: Comparaci\u00f3n autom\u00e1tica de +15 algoritmos.\n",
    "\n",
    "## \ud83d\udcc2 Entradas y Salidas\n",
    "- **Input**: `data/02_intermediate/process_data.parquet` (Datos limpios).\n",
    "- **Output**: `models/best_pipeline.pkl` (Modelo serializado listo para producci\u00f3n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuraci\u00f3n del Entorno\n",
    "\n",
    "Definimos par\u00e1metros globales.\n",
    "- **SAMPLE_FRAC**: Porcentaje de datos a usar. Para pruebas r\u00e1pidas usamos `0.5`, para el modelo final debe ser `1.0`.\n",
    "- **Rutas**: Ubicaci\u00f3n de datos y donde se guardar\u00e1n los artefactos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pycaret.classification import *\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "SAMPLE_FRAC = 0.5  # Set to 1.0 for full training\n",
    "DATA_PATH = \"../data/02_intermediate/process_data.parquet\"\n",
    "MODEL_DIR = \"../models\"\n",
    "MODEL_NAME = \"best_pipeline\"\n",
    "CONFIG_PATH = \"../models/model_config.json\"\n",
    "\n",
    "print(f\"Running Training with SAMPLE_FRAC = {SAMPLE_FRAC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Filtrado de Datos\n",
    "\n",
    "Cargamos el dataset y aplicamos el esquema definido en `model_config.json`.\n",
    "Es vital entrenar **solo** con las columnas que estar\u00e1n disponibles en la aplicaci\u00f3n final (Features + Target), descartando metadatos o IDs que causar\u00edan *data leakage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. LOAD DATA\n",
    "# ==========================================\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Data file not found at {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Original Data Shape: {df.shape}\")\n",
    "\n",
    "# Load Schema Config\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "features = config['features']\n",
    "target = config['target']\n",
    "numeric_features = config['numeric_features']\n",
    "categorical_features = config['categorical_features']\n",
    "\n",
    "# Filter only relevant columns\n",
    "df = df[features + [target]]\n",
    "\n",
    "if SAMPLE_FRAC < 1.0:\n",
    "    df = df.sample(frac=SAMPLE_FRAC, random_state=42)\n",
    "    print(f\"Sampled Data Shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"Using Full Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuraci\u00f3n del Experimento (Setup)\n",
    "\n",
    "La funci\u00f3n `setup()` inicializa el entorno de PyCaret y crea el pipeline de transformaci\u00f3n.\n",
    "- **normalize=True**: Escala las variables para que tengan rangos comparables. Usamos `RobustScaler` para ser resilientes a outliers.\n",
    "- **remove_outliers=True**: Elimina anomal\u00edas estad\u00edsticas que podr\u00edan sesgar el modelo.\n",
    "- **fix_imbalance=True**: Aplica SMOTE para generar muestras sint\u00e9ticas de la clase minoritaria (Enfermos), mejorando el aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. SETUP PYCARET\n",
    "# ==========================================\n",
    "# normalize=True (RobustScaler)\n",
    "# remove_outliers=True\n",
    "# fix_imbalance=True\n",
    "\n",
    "exp = setup(\n",
    "    data=df,\n",
    "    target=target,\n",
    "    numeric_features=numeric_features,\n",
    "    categorical_features=categorical_features,\n",
    "    normalize=True,\n",
    "    normalize_method='robust',\n",
    "    remove_outliers=True,\n",
    "    fix_imbalance=True,\n",
    "    session_id=123,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparaci\u00f3n y Selecci\u00f3n de Modelos\n",
    "\n",
    "Entrenamos m\u00faltiples algoritmos (Logistic Regression, XGBoost, Random Forest, etc.) con validaci\u00f3n cruzada (Cross-Validation).\n",
    "**M\u00e9trica Clave: Recall**. Buscamos maximizar la capacidad del modelo para detectar casos positivos reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. COMPARE & TRAIN\n",
    "# ==========================================\n",
    "# Optimizing for Recall (Sensitivity) as per medical requirements\n",
    "best_model = compare_models(sort='Recall')\n",
    "print(\"Best Model Found:\")\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Finalizaci\u00f3n y Persistencia\n",
    "\n",
    "Una vez seleccionado el mejor modelo:\n",
    "1. **Finalize**: Se re-entrena el modelo utilizando el 100% de los datos (incluyendo el set de prueba reservado anteriormente).\n",
    "2. **Save**: Se guarda el pipeline completo (preprocesamiento + modelo) en un archivo `.pkl` para su despliegue en la API/Streamlit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. FINALIZE & SAVE\n",
    "# ==========================================\n",
    "final_model = finalize_model(best_model)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "save_path = os.path.join(MODEL_DIR, MODEL_NAME)\n",
    "save_model(final_model, save_path)\n",
    "print(f\"Model saved successfully to {save_path}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}