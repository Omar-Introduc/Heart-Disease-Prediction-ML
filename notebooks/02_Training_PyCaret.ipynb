{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56543283",
   "metadata": {},
   "source": [
    "# ü§ñ Entrenamiento del Modelo Predictivo (PyCaret)\n",
    "\n",
    "## üéØ Objetivo\n",
    "Este notebook orquesta el pipeline de entrenamiento de Machine Learning utilizando **PyCaret**.\n",
    "El objetivo es encontrar y optimizar el mejor algoritmo capaz de predecir la probabilidad de **Enfermedad Card√≠aca** bas√°ndose en biomarcadores cl√≠nicos.\n",
    "\n",
    "## ‚öôÔ∏è Estrategia de Modelado\n",
    "1. **Preprocesamiento Robusto**: Normalizaci√≥n y manejo de outliers.\n",
    "2. **Balanceo de Clases**: Uso de t√©cnicas (SMOTE) para mitigar el desbalance entre pacientes sanos y enfermos.\n",
    "3. **Optimizaci√≥n de Recall**: Priorizamos la **Sensibilidad (Recall)** sobre la Precisi√≥n.\n",
    "   - *Contexto M√©dico*: Es peor no detectar a un enfermo (Falso Negativo) que alarmar a un sano (Falso Positivo).\n",
    "4. **Selecci√≥n de Modelos**: Comparaci√≥n autom√°tica de +15 algoritmos.\n",
    "\n",
    "## üìÇ Entradas y Salidas\n",
    "- **Input**: `data/02_intermediate/process_data.parquet` (Datos limpios).\n",
    "- **Output**: `models/best_pipeline.pkl` (Modelo serializado listo para producci√≥n)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f1336",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n del Entorno\n",
    "\n",
    "Definimos par√°metros globales.\n",
    "- **SAMPLE_FRAC**: Porcentaje de datos a usar. Para pruebas r√°pidas usamos `0.5`, para el modelo final debe ser `1.0`.\n",
    "- **Rutas**: Ubicaci√≥n de datos y donde se guardar√°n los artefactos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c552d92",
   "metadata": {},
   "source": [
    "### üîπ Paso 1: Configuraci√≥n del Entorno y Constantes\n",
    "Inicializamos el entorno de trabajo importando **PyCaret** y definiendo constantes cr√≠ticas:\n",
    "- `SAMPLE_FRAC`: Controla el muestreo de datos. Usamos 0.5 (50%) para iteraciones r√°pidas de desarrollo, pero se debe cambiar a 1.0 para el entrenamiento final.\n",
    "- `DATA_PATH` y `MODEL_DIR`: Definen las rutas de entrada de datos y salida del modelo, asegurando una estructura de proyecto ordenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ba014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Training with SAMPLE_FRAC = 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pycaret.classification import *\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "SAMPLE_FRAC = 0.01  # Modified for quick test\n",
    "DATA_PATH = \"../data/02_intermediate/process_data.parquet\"\n",
    "MODEL_DIR = \"../models\"\n",
    "MODEL_NAME = \"best_pipeline\"\n",
    "CONFIG_PATH = \"../models/model_config.json\"\n",
    "\n",
    "print(f\"Running Training with SAMPLE_FRAC = {SAMPLE_FRAC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164db18",
   "metadata": {},
   "source": [
    "## 2. Carga y Filtrado de Datos\n",
    "\n",
    "Cargamos el dataset y aplicamos el esquema definido en `model_config.json`.\n",
    "Es vital entrenar **solo** con las columnas que estar√°n disponibles en la aplicaci√≥n final (Features + Target), descartando metadatos o IDs que causar√≠an *data leakage*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f616e",
   "metadata": {},
   "source": [
    "### üîπ Paso 2: Carga y Selecci√≥n de Features (Data Loading)\n",
    "Cargamos el dataset procesado y aplicamos un filtro estricto de columnas basado en `model_config.json`.\n",
    "**Importante**:\n",
    "- Solo cargamos las columnas definidas como `features` y el `target`.\n",
    "- Esto act√∫a como una barrera de seguridad contra el *data leakage*, asegurando que el modelo no vea variables que no estar√°n disponibles en producci√≥n (como IDs de pacientes o fechas de procesamiento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3de049e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (43695, 29)\n",
      "Config file not found at ../models/model_config.json. Using default target and inferring features.\n",
      "Sampled Data Shape: (437, 29)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. LOAD DATA\n",
    "# ==========================================\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Data file not found at {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Original Data Shape: {df.shape}\")\n",
    "\n",
    "# Load Schema Config\n",
    "if os.path.exists(CONFIG_PATH):\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    features = config['numeric_features'] + config['categorical_features']\n",
    "    target = config['target']\n",
    "    numeric_features = config['numeric_features']\n",
    "    categorical_features = config['categorical_features']\n",
    "\n",
    "    # Filter only relevant columns\n",
    "    df = df[features + [target]]\n",
    "else:\n",
    "    print(f\"Config file not found at {CONFIG_PATH}. Using default target and inferring features.\")\n",
    "    target = 'HeartDisease'\n",
    "    numeric_features = None\n",
    "    categorical_features = None\n",
    "    # We do NOT filter df here because we don't know the exact features yet.\n",
    "    # PyCaret will use all columns in df except target as features.\n",
    "\n",
    "if SAMPLE_FRAC < 1.0:\n",
    "    df = df.sample(frac=SAMPLE_FRAC, random_state=42)\n",
    "    print(f\"Sampled Data Shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"Using Full Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07984a9c",
   "metadata": {},
   "source": [
    "## 3. Configuraci√≥n del Experimento (Setup)\n",
    "\n",
    "La funci√≥n `setup()` inicializa el entorno de PyCaret y crea el pipeline de transformaci√≥n.\n",
    "- **normalize=True**: Escala las variables para que tengan rangos comparables. Usamos `RobustScaler` para ser resilientes a outliers.\n",
    "- **remove_outliers=True**: Elimina anomal√≠as estad√≠sticas que podr√≠an sesgar el modelo.\n",
    "- **fix_imbalance=True**: Aplica SMOTE para generar muestras sint√©ticas de la clase minoritaria (Enfermos), mejorando el aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad53483",
   "metadata": {},
   "source": [
    "### üîπ Paso 3: Inicializaci√≥n del Experimento (PyCaret Setup)\n",
    "Configuramos el pipeline de preprocesamiento autom√°tico con `setup()`. Aqu√≠ definimos la \"magia\" de PyCaret:\n",
    "- **Normalizaci√≥n**: Aplicamos `RobustScaler` (`normalize_method='robust'`) para escalar los datos manejando bien los outliers t√≠picos de datos cl√≠nicos.\n",
    "- **Balanceo de Clases**: Activamos `fix_imbalance=True` (SMOTE) para generar datos sint√©ticos de la clase minoritaria (pacientes enfermos), evitando que el modelo se sesgue hacia la clase mayoritaria (sanos).\n",
    "- **Tipos de Datos**: Definimos expl√≠citamente cu√°les son num√©ricas y cu√°les categ√≥ricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "186d56f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8f739_row8_col1, #T_8f739_row12_col1, #T_8f739_row14_col1, #T_8f739_row16_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8f739\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8f739_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_8f739_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_8f739_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_8f739_row0_col1\" class=\"data row0 col1\" >42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_8f739_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_8f739_row1_col1\" class=\"data row1 col1\" >HeartDisease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_8f739_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_8f739_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_8f739_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_8f739_row3_col1\" class=\"data row3 col1\" >(437, 29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_8f739_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_8f739_row4_col1\" class=\"data row4 col1\" >(698, 29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_8f739_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_8f739_row5_col1\" class=\"data row5 col1\" >(566, 29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_8f739_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_8f739_row6_col1\" class=\"data row6 col1\" >(132, 29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_8f739_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_8f739_row7_col1\" class=\"data row7 col1\" >28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_8f739_row8_col0\" class=\"data row8 col0\" >Preprocess</td>\n",
       "      <td id=\"T_8f739_row8_col1\" class=\"data row8 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_8f739_row9_col0\" class=\"data row9 col0\" >Imputation type</td>\n",
       "      <td id=\"T_8f739_row9_col1\" class=\"data row9 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_8f739_row10_col0\" class=\"data row10 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_8f739_row10_col1\" class=\"data row10 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_8f739_row11_col0\" class=\"data row11 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_8f739_row11_col1\" class=\"data row11 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_8f739_row12_col0\" class=\"data row12 col0\" >Remove outliers</td>\n",
       "      <td id=\"T_8f739_row12_col1\" class=\"data row12 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_8f739_row13_col0\" class=\"data row13 col0\" >Outliers threshold</td>\n",
       "      <td id=\"T_8f739_row13_col1\" class=\"data row13 col1\" >0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_8f739_row14_col0\" class=\"data row14 col0\" >Fix imbalance</td>\n",
       "      <td id=\"T_8f739_row14_col1\" class=\"data row14 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_8f739_row15_col0\" class=\"data row15 col0\" >Fix imbalance method</td>\n",
       "      <td id=\"T_8f739_row15_col1\" class=\"data row15 col1\" >SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_8f739_row16_col0\" class=\"data row16 col0\" >Normalize</td>\n",
       "      <td id=\"T_8f739_row16_col1\" class=\"data row16 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_8f739_row17_col0\" class=\"data row17 col0\" >Normalize method</td>\n",
       "      <td id=\"T_8f739_row17_col1\" class=\"data row17 col1\" >robust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_8f739_row18_col0\" class=\"data row18 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_8f739_row18_col1\" class=\"data row18 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_8f739_row19_col0\" class=\"data row19 col0\" >Fold Number</td>\n",
       "      <td id=\"T_8f739_row19_col1\" class=\"data row19 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_8f739_row20_col0\" class=\"data row20 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_8f739_row20_col1\" class=\"data row20 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_8f739_row21_col0\" class=\"data row21 col0\" >Use GPU</td>\n",
       "      <td id=\"T_8f739_row21_col1\" class=\"data row21 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_8f739_row22_col0\" class=\"data row22 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_8f739_row22_col1\" class=\"data row22 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_8f739_row23_col0\" class=\"data row23 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_8f739_row23_col1\" class=\"data row23 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8f739_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_8f739_row24_col0\" class=\"data row24 col0\" >USI</td>\n",
       "      <td id=\"T_8f739_row24_col1\" class=\"data row24 col1\" >c9e5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x217d12855a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. SETUP PYCARET\n",
    "# ==========================================\n",
    "# normalize=True (RobustScaler)\n",
    "# remove_outliers=True\n",
    "# fix_imbalance=True\n",
    "\n",
    "# Modifica esta celda en tu notebook\n",
    "exp = setup(\n",
    "    data=df,\n",
    "    target=target,\n",
    "    numeric_features=numeric_features,\n",
    "    categorical_features=categorical_features,\n",
    "    normalize=True,\n",
    "    normalize_method='robust',\n",
    "    remove_outliers=True,\n",
    "    fix_imbalance=True,\n",
    "    session_id=42,\n",
    "    verbose=True,\n",
    "    n_jobs=1  # <--- AGREGA ESTA L√çNEA (Soluciona el error de _winapi.CreateProcess)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f7ad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost versi√≥n: 3.1.2 - OK\n",
      "LightGBM versi√≥n: 4.6.0 - OK\n"
     ]
    }
   ],
   "source": [
    "# Verificaci√≥n de librer√≠as\n",
    "try:\n",
    "    import xgboost\n",
    "    print(f\"XGBoost versi√≥n: {xgboost.__version__} - OK\")\n",
    "    import lightgbm\n",
    "    print(f\"LightGBM versi√≥n: {lightgbm.__version__} - OK\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error de importaci√≥n: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a9351",
   "metadata": {},
   "source": [
    "## 4. Comparaci√≥n y Selecci√≥n de Modelos\n",
    "\n",
    "Entrenamos m√∫ltiples algoritmos (Logistic Regression, XGBoost, Random Forest, etc.) con validaci√≥n cruzada (Cross-Validation).\n",
    "**M√©trica Clave: Recall**. Buscamos maximizar la capacidad del modelo para detectar casos positivos reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd7e989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3c38e\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x217c40006d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Models: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 3. SELECCI√ìN DE MODELO (Model Selection)\n",
    "# ==========================================\n",
    "# Estrategia: Comparar modelos basados en √°rboles y seleccionar los Top 3 para tuning.\n",
    "# Restricci√≥n: Solo √°rboles (XGBoost, LightGBM)\n",
    "# M√©trica: Recall (Sensibilidad)\n",
    "\n",
    "top_models = compare_models(\n",
    "    include=['xgboost', 'lightgbm'],\n",
    "    sort='Recall',\n",
    "    n_select=3,\n",
    "    verbose=True # Cambia a True para ver el progreso paso a paso\n",
    ")\n",
    "print(f\"Top 3 Models: {top_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ebbabd",
   "metadata": {},
   "source": [
    "### üîπ Paso 4.1: Optimizaci√≥n Profunda de Hiperpar√°metros\n",
    "Ya tenemos el mejor candidato ('best_model'). Ahora, no nos conformamos con sus par√°metros por defecto. \n",
    "Ejecutamos un proceso de **Tuning Exhaustivo**:\n",
    "- **optimize='Recall'**: El algoritmo de b√∫squeda intentar√° maximizar espec√≠ficamente la sensibilidad.\n",
    "- **n_iter=50**: Probamos 50 combinaciones de hiperpar√°metros distintas. ¬øPor qu√© 50? En medicina, la diferencia entre un recall del 85% y 87% puede significar salvar m√°s vidas (menos Falsos Negativos). Una b√∫squeda superficial (n_iter=10) podr√≠a perder el √≥ptimo global.\n",
    "- **choose_better=True**: Si despu√©s de tunear el modelo empeora, nos quedamos con la versi√≥n original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3.1 DEEP HYPERPARAMETER TUNING\n",
    "# ==========================================\n",
    "best_model = top_models[0]\n",
    "print(f\"\\n--- Tuning Best Model: {type(best_model).__name__} ---\")\n",
    "\n",
    "# Tuning loop\n",
    "# n_iter=2 para b√∫squeda exhaustiva (Deep Search)\n",
    "tuned_model = tune_model(\n",
    "    best_model, \n",
    "    optimize='Recall', \n",
    "    n_iter=2, \n",
    "    choose_better=True, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nFINAL BEST MODEL SELECTED: {tuned_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb48725",
   "metadata": {},
   "source": [
    "### üîπ Paso 4.2: Optimizaci√≥n de Umbral de Decisi√≥n\n",
    "\n",
    "Implementamos la estrategia **Precision-Constrained Recall Maximization**.\n",
    "Buscamos el umbral que maximice el Recall, sujeto a que la Precisi√≥n sea >= 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa8fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Estrategia de Umbral de Seguridad Cl√≠nica\n",
    "print(\"\\n--- Optimizando Umbral de Decisi√≥n ---\")\n",
    "# Generar probabilidades en el set de validaci√≥n (hold-out)\n",
    "predictions = predict_model(tuned_model, raw_score=True, verbose=False)\n",
    "\n",
    "# Identificar columnas de score y target real\n",
    "target_col = get_config('target_param')\n",
    "y_true = predictions[target_col]\n",
    "\n",
    "# Buscar columna de score para clase positiva (1)\n",
    "score_cols = [c for c in predictions.columns if 'score' in c]\n",
    "if any('1' in c for c in score_cols):\n",
    "    score_col = [c for c in score_cols if '1' in c][0]\n",
    "else:\n",
    "    score_col = score_cols[0]\n",
    "\n",
    "y_scores = predictions[score_col]\n",
    "\n",
    "# Iterar umbrales\n",
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "results = []\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_scores >= t).astype(int)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    results.append({'Threshold': t, 'Precision': prec, 'Recall': rec})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Filtrar zona segura: Precision >= 0.4\n",
    "safe_zone = results_df[results_df['Precision'] >= 0.4]\n",
    "\n",
    "if not safe_zone.empty:\n",
    "    # Seleccionar el umbral con mayor Recall dentro de la zona segura\n",
    "    # (Generalmente el umbral m√°s bajo de la zona)\n",
    "    best_row = safe_zone.sort_values('Recall', ascending=False).iloc[0]\n",
    "    optimal_threshold = best_row['Threshold']\n",
    "    print(f\"‚úÖ Umbral √ìptimo Encontrado: {optimal_threshold:.2f}\")\n",
    "    print(f\"   M√©tricas Esperadas -> Recall: {best_row['Recall']:.4f} | Precision: {best_row['Precision']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se alcanz√≥ la zona segura (Precision >= 0.4). Se usar√° umbral por defecto (0.5).\")\n",
    "    optimal_threshold = 0.5\n",
    "\n",
    "# Visualizaci√≥n de la Curva de Seguridad\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['Threshold'], results_df['Precision'], label='Precision', color='blue')\n",
    "plt.plot(results_df['Threshold'], results_df['Recall'], label='Recall', color='green')\n",
    "plt.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimum ({optimal_threshold:.2f})')\n",
    "\n",
    "# Sombrear zona segura si existe\n",
    "if not safe_zone.empty:\n",
    "    plt.axvspan(safe_zone['Threshold'].min(), safe_zone['Threshold'].max(), alpha=0.1, color='green', label='Zona Segura (Prec>=0.4)')\n",
    "\n",
    "plt.title(f\"Curva de Seguridad Cl√≠nica: Selecci√≥n de Umbral para {type(tuned_model).__name__}\")\n",
    "plt.xlabel(\"Umbral de Decisi√≥n\")\n",
    "plt.ylabel(\"M√©trica\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628056e5",
   "metadata": {},
   "source": [
    "## 5. Finalizaci√≥n y Persistencia\n",
    "\n",
    "Una vez seleccionado el mejor modelo:\n",
    "1. **Finalize**: Se re-entrena el modelo utilizando el 100% de los datos (incluyendo el set de prueba reservado anteriormente).\n",
    "2. **Save**: Se guarda el pipeline completo (preprocesamiento + modelo) en un archivo `.pkl` para su despliegue en la API/Streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c900cad",
   "metadata": {},
   "source": [
    "### üîπ Paso 5: Finalizaci√≥n y Serializaci√≥n del Modelo\n",
    "Una vez seleccionado el mejor algoritmo:\n",
    "1.  **Finalize**: Re-entrenamos el modelo utilizando **todos** los datos disponibles (incluyendo el set de validaci√≥n que PyCaret retuvo internamente).\n",
    "2.  **Save**: Guardamos el pipeline completo como un archivo `.pkl` en el directorio `models/`. Este archivo contiene tanto el modelo predictivo como las transformaciones de datos (escalado, imputaci√≥n), listo para ser consumido por la API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca728fb1",
   "metadata": {},
   "source": [
    "## 4.5 Explicabilidad del Modelo (SHAP)\n",
    "\n",
    "Validamos que el modelo no tome decisiones basadas en artefactos o sesgos. Generamos el **SHAP Summary Plot** para visualizar las variables m√°s impactantes.\n",
    "Esto es un requisito de **Transparencia Algor√≠tmica** para la auditor√≠a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar SHAP Summary Plot\n",
    "print(\"Generando explicaciones SHAP...\")\n",
    "try:\n",
    "    interpret_model(tuned_model, plot='summary')\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo generar el gr√°fico SHAP (probablemente el modelo no lo soporte nativamente o falte librer√≠a): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c1f7412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n",
      "Model saved successfully to ../models\\best_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. FINALIZE & SAVE\n",
    "# ==========================================\n",
    "final_model = finalize_model(tuned_model)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "save_path = os.path.join(MODEL_DIR, MODEL_NAME)\n",
    "save_model(final_model, save_path)\n",
    "print(f\"Model saved successfully to {save_path}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
