{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56543283",
   "metadata": {},
   "source": [
    "# \ud83e\udd16 Entrenamiento del Modelo Predictivo (PyCaret)\n",
    "\n",
    "## \ud83c\udfaf Objetivo\n",
    "Este notebook orquesta el pipeline de entrenamiento de Machine Learning utilizando **PyCaret**.\n",
    "El objetivo es encontrar y optimizar el mejor algoritmo capaz de predecir la probabilidad de **Enfermedad Card\u00edaca** bas\u00e1ndose en biomarcadores cl\u00ednicos.\n",
    "\n",
    "## \u2699\ufe0f Estrategia de Modelado\n",
    "1. **Preprocesamiento Robusto**: Normalizaci\u00f3n y manejo de outliers.\n",
    "2. **Balanceo de Clases**: Uso de t\u00e9cnicas (SMOTE) para mitigar el desbalance entre pacientes sanos y enfermos.\n",
    "3. **Optimizaci\u00f3n de Recall**: Priorizamos la **Sensibilidad (Recall)** sobre la Precisi\u00f3n.\n",
    "   - *Contexto M\u00e9dico*: Es peor no detectar a un enfermo (Falso Negativo) que alarmar a un sano (Falso Positivo).\n",
    "4. **Selecci\u00f3n de Modelos**: Comparaci\u00f3n autom\u00e1tica de +15 algoritmos.\n",
    "\n",
    "## \ud83d\udcc2 Entradas y Salidas\n",
    "- **Input**: `data/02_intermediate/process_data.parquet` (Datos limpios).\n",
    "- **Output**: `models/best_pipeline.pkl` (Modelo serializado listo para producci\u00f3n)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f1336",
   "metadata": {},
   "source": [
    "## 1. Configuraci\u00f3n del Entorno\n",
    "\n",
    "Definimos par\u00e1metros globales.\n",
    "- **SAMPLE_FRAC**: Porcentaje de datos a usar. Para pruebas r\u00e1pidas usamos `0.5`, para el modelo final debe ser `1.0`.\n",
    "- **Rutas**: Ubicaci\u00f3n de datos y donde se guardar\u00e1n los artefactos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c552d92",
   "metadata": {},
   "source": [
    "### \ud83d\udd39 Paso 1: Configuraci\u00f3n del Entorno y Constantes\n",
    "Inicializamos el entorno de trabajo importando **PyCaret** y definiendo constantes cr\u00edticas:\n",
    "- `SAMPLE_FRAC`: Controla el muestreo de datos. Usamos 0.5 (50%) para iteraciones r\u00e1pidas de desarrollo, pero se debe cambiar a 1.0 para el entrenamiento final.\n",
    "- `DATA_PATH` y `MODEL_DIR`: Definen las rutas de entrada de datos y salida del modelo, asegurando una estructura de proyecto ordenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ba014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Training with SAMPLE_FRAC = 0.05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pycaret.classification import *\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "SAMPLE_FRAC = 0.01  # Set to 1.0 for full training\n",
    "DATA_PATH = \"../data/02_intermediate/process_data.parquet\"\n",
    "MODEL_DIR = \"../models\"\n",
    "MODEL_NAME = \"best_pipeline\"\n",
    "CONFIG_PATH = \"../models/model_config.json\"\n",
    "\n",
    "print(f\"Running Training with SAMPLE_FRAC = {SAMPLE_FRAC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164db18",
   "metadata": {},
   "source": [
    "## 2. Carga y Filtrado de Datos\n",
    "\n",
    "Cargamos el dataset y aplicamos el esquema definido en `model_config.json`.\n",
    "Es vital entrenar **solo** con las columnas que estar\u00e1n disponibles en la aplicaci\u00f3n final (Features + Target), descartando metadatos o IDs que causar\u00edan *data leakage*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f616e",
   "metadata": {},
   "source": [
    "### \ud83d\udd39 Paso 2: Carga y Selecci\u00f3n de Features (Data Loading)\n",
    "Cargamos el dataset procesado y aplicamos un filtro estricto de columnas basado en `model_config.json`.\n",
    "**Importante**:\n",
    "- Solo cargamos las columnas definidas como `features` y el `target`.\n",
    "- Esto act\u00faa como una barrera de seguridad contra el *data leakage*, asegurando que el modelo no vea variables que no estar\u00e1n disponibles en producci\u00f3n (como IDs de pacientes o fechas de procesamiento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3de049e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (43695, 28)\n",
      "Sampled Data Shape: (2185, 27)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. LOAD DATA\n",
    "# ==========================================\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Data file not found at {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Original Data Shape: {df.shape}\")\n",
    "\n",
    "# Load Schema Config\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "features = config['numeric_features'] + config['categorical_features']\n",
    "target = config['target']\n",
    "numeric_features = config['numeric_features']\n",
    "categorical_features = config['categorical_features']\n",
    "\n",
    "# Filter only relevant columns\n",
    "df = df[features + [target]]\n",
    "\n",
    "if SAMPLE_FRAC < 1.0:\n",
    "    df = df.sample(frac=SAMPLE_FRAC, random_state=42)\n",
    "    print(f\"Sampled Data Shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"Using Full Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07984a9c",
   "metadata": {},
   "source": [
    "## 3. Configuraci\u00f3n del Experimento (Setup)\n",
    "\n",
    "La funci\u00f3n `setup()` inicializa el entorno de PyCaret y crea el pipeline de transformaci\u00f3n.\n",
    "- **normalize=True**: Escala las variables para que tengan rangos comparables. Usamos `RobustScaler` para ser resilientes a outliers.\n",
    "- **remove_outliers=True**: Elimina anomal\u00edas estad\u00edsticas que podr\u00edan sesgar el modelo.\n",
    "- **fix_imbalance=True**: Aplica SMOTE para generar muestras sint\u00e9ticas de la clase minoritaria (Enfermos), mejorando el aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad53483",
   "metadata": {},
   "source": [
    "### \ud83d\udd39 Paso 3: Inicializaci\u00f3n del Experimento (PyCaret Setup)\n",
    "Configuramos el pipeline de preprocesamiento autom\u00e1tico con `setup()`. Aqu\u00ed definimos la \"magia\" de PyCaret:\n",
    "- **Normalizaci\u00f3n**: Aplicamos `RobustScaler` (`normalize_method='robust'`) para escalar los datos manejando bien los outliers t\u00edpicos de datos cl\u00ednicos.\n",
    "- **Balanceo de Clases**: Activamos `fix_imbalance=True` (SMOTE) para generar datos sint\u00e9ticos de la clase minoritaria (pacientes enfermos), evitando que el modelo se sesgue hacia la clase mayoritaria (sanos).\n",
    "- **Tipos de Datos**: Definimos expl\u00edcitamente cu\u00e1les son num\u00e9ricas y cu\u00e1les categ\u00f3ricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "186d56f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\OMAR\\miniconda3\\envs\\xgb_env\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\OMAR\\miniconda3\\envs\\xgb_env\\lib\\subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\OMAR\\miniconda3\\envs\\xgb_env\\lib\\subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\OMAR\\miniconda3\\envs\\xgb_env\\lib\\subprocess.py\", line 1456, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_58fd9_row10_col1, #T_58fd9_row16_col1, #T_58fd9_row18_col1, #T_58fd9_row20_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_58fd9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_58fd9_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_58fd9_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_58fd9_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_58fd9_row0_col1\" class=\"data row0 col1\" >123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_58fd9_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_58fd9_row1_col1\" class=\"data row1 col1\" >HeartDisease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_58fd9_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_58fd9_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_58fd9_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_58fd9_row3_col1\" class=\"data row3 col1\" >(2185, 27)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_58fd9_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_58fd9_row4_col1\" class=\"data row4 col1\" >(3484, 36)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_58fd9_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_58fd9_row5_col1\" class=\"data row5 col1\" >(2828, 36)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_58fd9_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_58fd9_row6_col1\" class=\"data row6 col1\" >(656, 36)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_58fd9_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_58fd9_row7_col1\" class=\"data row7 col1\" >19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_58fd9_row8_col0\" class=\"data row8 col0\" >Categorical features</td>\n",
       "      <td id=\"T_58fd9_row8_col1\" class=\"data row8 col1\" >7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_58fd9_row9_col0\" class=\"data row9 col0\" >Rows with missing values</td>\n",
       "      <td id=\"T_58fd9_row9_col1\" class=\"data row9 col1\" >87.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_58fd9_row10_col0\" class=\"data row10 col0\" >Preprocess</td>\n",
       "      <td id=\"T_58fd9_row10_col1\" class=\"data row10 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_58fd9_row11_col0\" class=\"data row11 col0\" >Imputation type</td>\n",
       "      <td id=\"T_58fd9_row11_col1\" class=\"data row11 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_58fd9_row12_col0\" class=\"data row12 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_58fd9_row12_col1\" class=\"data row12 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_58fd9_row13_col0\" class=\"data row13 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_58fd9_row13_col1\" class=\"data row13 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_58fd9_row14_col0\" class=\"data row14 col0\" >Maximum one-hot encoding</td>\n",
       "      <td id=\"T_58fd9_row14_col1\" class=\"data row14 col1\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_58fd9_row15_col0\" class=\"data row15 col0\" >Encoding method</td>\n",
       "      <td id=\"T_58fd9_row15_col1\" class=\"data row15 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_58fd9_row16_col0\" class=\"data row16 col0\" >Remove outliers</td>\n",
       "      <td id=\"T_58fd9_row16_col1\" class=\"data row16 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_58fd9_row17_col0\" class=\"data row17 col0\" >Outliers threshold</td>\n",
       "      <td id=\"T_58fd9_row17_col1\" class=\"data row17 col1\" >0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_58fd9_row18_col0\" class=\"data row18 col0\" >Fix imbalance</td>\n",
       "      <td id=\"T_58fd9_row18_col1\" class=\"data row18 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_58fd9_row19_col0\" class=\"data row19 col0\" >Fix imbalance method</td>\n",
       "      <td id=\"T_58fd9_row19_col1\" class=\"data row19 col1\" >SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_58fd9_row20_col0\" class=\"data row20 col0\" >Normalize</td>\n",
       "      <td id=\"T_58fd9_row20_col1\" class=\"data row20 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_58fd9_row21_col0\" class=\"data row21 col0\" >Normalize method</td>\n",
       "      <td id=\"T_58fd9_row21_col1\" class=\"data row21 col1\" >robust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_58fd9_row22_col0\" class=\"data row22 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_58fd9_row22_col1\" class=\"data row22 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_58fd9_row23_col0\" class=\"data row23 col0\" >Fold Number</td>\n",
       "      <td id=\"T_58fd9_row23_col1\" class=\"data row23 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_58fd9_row24_col0\" class=\"data row24 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_58fd9_row24_col1\" class=\"data row24 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_58fd9_row25_col0\" class=\"data row25 col0\" >Use GPU</td>\n",
       "      <td id=\"T_58fd9_row25_col1\" class=\"data row25 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_58fd9_row26_col0\" class=\"data row26 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_58fd9_row26_col1\" class=\"data row26 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_58fd9_row27_col0\" class=\"data row27 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_58fd9_row27_col1\" class=\"data row27 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_58fd9_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_58fd9_row28_col0\" class=\"data row28 col0\" >USI</td>\n",
       "      <td id=\"T_58fd9_row28_col1\" class=\"data row28 col1\" >7743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25ee8094460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. SETUP PYCARET\n",
    "# ==========================================\n",
    "# normalize=True (RobustScaler)\n",
    "# remove_outliers=True\n",
    "# fix_imbalance=True\n",
    "\n",
    "exp = setup(\n",
    "    data=df,\n",
    "    target=target,\n",
    "    numeric_features=numeric_features,\n",
    "    categorical_features=categorical_features,\n",
    "    normalize=True,\n",
    "    normalize_method='robust',\n",
    "    remove_outliers=True,\n",
    "    fix_imbalance=True,\n",
    "    session_id=42,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a9351",
   "metadata": {},
   "source": [
    "## 4. Comparaci\u00f3n y Selecci\u00f3n de Modelos\n",
    "\n",
    "Entrenamos m\u00faltiples algoritmos (Logistic Regression, XGBoost, Random Forest, etc.) con validaci\u00f3n cruzada (Cross-Validation).\n",
    "**M\u00e9trica Clave: Recall**. Buscamos maximizar la capacidad del modelo para detectar casos positivos reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 3. SELECCI\u00d3N DE MODELO (Model Selection)\n",
    "# ==========================================\n",
    "# Estrategia: Comparar modelos basados en \u00e1rboles y seleccionar los Top 3 para tuning.\n",
    "# Restricci\u00f3n: Solo \u00e1rboles (XGBoost, LightGBM)\n",
    "# M\u00e9trica: Recall (Sensibilidad)\n",
    "\n",
    "top_models = compare_models(\n",
    "    include=['xgboost', 'lightgbm'],\n",
    "    sort='Recall',\n",
    "    n_select=1,\n",
    "    verbose=False\n",
    ")\n",
    "print(f\"Top 3 Models: {top_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ebbabd",
   "metadata": {},
   "source": [
    "### \ud83d\udd39 Paso 4.1: Optimizaci\u00f3n Profunda de Hiperpar\u00e1metros\n",
    "Ya tenemos el mejor candidato ('best_model'). Ahora, no nos conformamos con sus par\u00e1metros por defecto. \n",
    "Ejecutamos un proceso de **Tuning Exhaustivo**:\n",
    "- **optimize='Recall'**: El algoritmo de b\u00fasqueda intentar\u00e1 maximizar espec\u00edficamente la sensibilidad.\n",
    "- **n_iter=50**: Probamos 50 combinaciones de hiperpar\u00e1metros distintas. \u00bfPor qu\u00e9 50? En medicina, la diferencia entre un recall del 85% y 87% puede significar salvar m\u00e1s vidas (menos Falsos Negativos). Una b\u00fasqueda superficial (n_iter=10) podr\u00eda perder el \u00f3ptimo global.\n",
    "- **choose_better=True**: Si despu\u00e9s de tunear el modelo empeora, nos quedamos con la versi\u00f3n original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b58526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3.1 DEEP HYPERPARAMETER TUNING\n",
    "# ==========================================\n",
    "best_model = top_models[0] if isinstance(top_models, list) else top_models\n",
    "print(f\"\\n--- Tuning Best Model: {type(best_model).__name__} ---\")\n",
    "\n",
    "# Tuning loop\n",
    "# n_iter=1 para b\u00fasqueda exhaustiva (Deep Search)\n",
    "tuned_model = tune_model(\n",
    "    best_model, \n",
    "    optimize='Recall', \n",
    "    n_iter=1, \n",
    "    choose_better=True, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nFINAL BEST MODEL SELECTED: {tuned_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb48725",
   "metadata": {},
   "source": [
    "### \ud83d\udd39 Paso 4.2: Optimizaci\u00f3n de Umbral de Decisi\u00f3n\n",
    "\n",
    "Implementamos la estrategia **Precision-Constrained Recall Maximization**.\n",
    "Buscamos el umbral que maximice el Recall, sujeto a que la Precisi\u00f3n sea >= 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa8fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Estrategia de Umbral de Seguridad Cl\u00ednica\n",
    "print(\"\\n--- Optimizando Umbral de Decisi\u00f3n ---\")\n",
    "# Generar probabilidades en el set de validaci\u00f3n (hold-out)\n",
    "predictions = predict_model(tuned_model, raw_score=True, verbose=False)\n",
    "\n",
    "# Identificar columnas de score y target real\n",
    "target_col = get_config('target_param')\n",
    "y_true = predictions[target_col]\n",
    "\n",
    "# Buscar columna de score para clase positiva (1)\n",
    "score_cols = [c for c in predictions.columns if 'score' in c]\n",
    "if any('1' in c for c in score_cols):\n",
    "    score_col = [c for c in score_cols if '1' in c][0]\n",
    "else:\n",
    "    score_col = score_cols[0]\n",
    "\n",
    "y_scores = predictions[score_col]\n",
    "\n",
    "# Iterar umbrales\n",
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "results = []\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (y_scores >= t).astype(int)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    results.append({'Threshold': t, 'Precision': prec, 'Recall': rec})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Filtrar zona segura: Precision >= 0.4\n",
    "safe_zone = results_df[results_df['Precision'] >= 0.4]\n",
    "\n",
    "if not safe_zone.empty:\n",
    "    # Seleccionar el umbral con mayor Recall dentro de la zona segura\n",
    "    # (Generalmente el umbral m\u00e1s bajo de la zona)\n",
    "    best_row = safe_zone.sort_values('Recall', ascending=False).iloc[0]\n",
    "    optimal_threshold = best_row['Threshold']\n",
    "    print(f\"\u2705 Umbral \u00d3ptimo Encontrado: {optimal_threshold:.2f}\")\n",
    "    print(f\"   M\u00e9tricas Esperadas -> Recall: {best_row['Recall']:.4f} | Precision: {best_row['Precision']:.4f}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f No se alcanz\u00f3 la zona segura (Precision >= 0.4). Se usar\u00e1 umbral por defecto (0.5).\")\n",
    "    optimal_threshold = 0.5\n",
    "\n",
    "# Visualizaci\u00f3n de la Curva de Seguridad\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['Threshold'], results_df['Precision'], label='Precision', color='blue')\n",
    "plt.plot(results_df['Threshold'], results_df['Recall'], label='Recall', color='green')\n",
    "plt.axvline(optimal_threshold, color='red', linestyle='--', label=f'Optimum ({optimal_threshold:.2f})')\n",
    "\n",
    "# Sombrear zona segura si existe\n",
    "if not safe_zone.empty:\n",
    "    plt.axvspan(safe_zone['Threshold'].min(), safe_zone['Threshold'].max(), alpha=0.1, color='green', label='Zona Segura (Prec>=0.4)')\n",
    "\n",
    "plt.title(f\"Curva de Seguridad Cl\u00ednica: Selecci\u00f3n de Umbral para {type(tuned_model).__name__}\")\n",
    "plt.xlabel(\"Umbral de Decisi\u00f3n\")\n",
    "plt.ylabel(\"M\u00e9trica\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628056e5",
   "metadata": {},
   "source": [
    "## 5. Finalizaci\u00f3n y Persistencia\n",
    "\n",
    "Una vez seleccionado el mejor modelo:\n",
    "1. **Finalize**: Se re-entrena el modelo utilizando el 100% de los datos (incluyendo el set de prueba reservado anteriormente).\n",
    "2. **Save**: Se guarda el pipeline completo (preprocesamiento + modelo) en un archivo `.pkl` para su despliegue en la API/Streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c900cad",
   "metadata": {},
   "source": [
    "### \ud83d\udd39 Paso 5: Finalizaci\u00f3n y Serializaci\u00f3n del Modelo\n",
    "Una vez seleccionado el mejor algoritmo:\n",
    "1.  **Finalize**: Re-entrenamos el modelo utilizando **todos** los datos disponibles (incluyendo el set de validaci\u00f3n que PyCaret retuvo internamente).\n",
    "2.  **Save**: Guardamos el pipeline completo como un archivo `.pkl` en el directorio `models/`. Este archivo contiene tanto el modelo predictivo como las transformaciones de datos (escalado, imputaci\u00f3n), listo para ser consumido por la API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca728fb1",
   "metadata": {},
   "source": [
    "## 4.5 Explicabilidad del Modelo (SHAP)\n",
    "\n",
    "Validamos que el modelo no tome decisiones basadas en artefactos o sesgos. Generamos el **SHAP Summary Plot** para visualizar las variables m\u00e1s impactantes.\n",
    "Esto es un requisito de **Transparencia Algor\u00edtmica** para la auditor\u00eda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar SHAP Summary Plot\n",
    "print(\"Generando explicaciones SHAP...\")\n",
    "try:\n",
    "    interpret_model(tuned_model, plot='summary')\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo generar el gr\u00e1fico SHAP (probablemente el modelo no lo soporte nativamente o falte librer\u00eda): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c1f7412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n",
      "Model saved successfully to ../models\\best_pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. FINALIZE & SAVE\n",
    "# ==========================================\n",
    "final_model = finalize_model(tuned_model)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "save_path = os.path.join(MODEL_DIR, MODEL_NAME)\n",
    "save_model(final_model, save_path)\n",
    "print(f\"Model saved successfully to {save_path}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}