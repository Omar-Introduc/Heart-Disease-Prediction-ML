{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 游뱄 Entrenamiento del Modelo Predictivo (PyCaret)\n",
    "\n",
    "## 游꿢 Objetivo\n",
    "Este notebook orquesta el pipeline de entrenamiento de Machine Learning utilizando **PyCaret**.\n",
    "El objetivo es encontrar y optimizar el mejor algoritmo capaz de predecir la probabilidad de **Enfermedad Card칤aca** bas치ndose en biomarcadores cl칤nicos.\n",
    "\n",
    "## 丘뙖잺 Estrategia de Modelado\n",
    "1. **Preprocesamiento Robusto**: Normalizaci칩n y manejo de outliers.\n",
    "2. **Balanceo de Clases**: Uso de t칠cnicas (SMOTE) para mitigar el desbalance entre pacientes sanos y enfermos.\n",
    "3. **Optimizaci칩n de Recall**: Priorizamos la **Sensibilidad (Recall)** sobre la Precisi칩n.\n",
    "   - *Contexto M칠dico*: Es peor no detectar a un enfermo (Falso Negativo) que alarmar a un sano (Falso Positivo).\n",
    "4. **Selecci칩n de Modelos**: Comparaci칩n autom치tica de +15 algoritmos.\n",
    "\n",
    "## 游늭 Entradas y Salidas\n",
    "- **Input**: `data/02_intermediate/process_data.parquet` (Datos limpios).\n",
    "- **Output**: `models/best_pipeline.pkl` (Modelo serializado listo para producci칩n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuraci칩n del Entorno\n",
    "\n",
    "Definimos par치metros globales.\n",
    "- **SAMPLE_FRAC**: Porcentaje de datos a usar. Para pruebas r치pidas usamos `0.5`, para el modelo final debe ser `1.0`.\n",
    "- **Rutas**: Ubicaci칩n de datos y donde se guardar치n los artefactos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 游댳 Paso 1: Configuraci칩n del Entorno y Constantes\n",
    "Inicializamos el entorno de trabajo importando **PyCaret** y definiendo constantes cr칤ticas:\n",
    "- `SAMPLE_FRAC`: Controla el muestreo de datos. Usamos 0.5 (50%) para iteraciones r치pidas de desarrollo, pero se debe cambiar a 1.0 para el entrenamiento final.\n",
    "- `DATA_PATH` y `MODEL_DIR`: Definen las rutas de entrada de datos y salida del modelo, asegurando una estructura de proyecto ordenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Training with SAMPLE_FRAC = 0.05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pycaret.classification import *\n",
    "import os\n",
    "import json\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "SAMPLE_FRAC = 0.05  # Set to 1.0 for full training\n",
    "DATA_PATH = \"../data/02_intermediate/process_data.parquet\"\n",
    "MODEL_DIR = \"../models\"\n",
    "MODEL_NAME = \"best_pipeline\"\n",
    "CONFIG_PATH = \"../models/model_config.json\"\n",
    "\n",
    "print(f\"Running Training with SAMPLE_FRAC = {SAMPLE_FRAC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Filtrado de Datos\n",
    "\n",
    "Cargamos el dataset y aplicamos el esquema definido en `model_config.json`.\n",
    "Es vital entrenar **solo** con las columnas que estar치n disponibles en la aplicaci칩n final (Features + Target), descartando metadatos o IDs que causar칤an *data leakage*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 游댳 Paso 2: Carga y Selecci칩n de Features (Data Loading)\n",
    "Cargamos el dataset procesado y aplicamos un filtro estricto de columnas basado en `model_config.json`.\n",
    "**Importante**:\n",
    "- Solo cargamos las columnas definidas como `features` y el `target`.\n",
    "- Esto act칰a como una barrera de seguridad contra el *data leakage*, asegurando que el modelo no vea variables que no estar치n disponibles en producci칩n (como IDs de pacientes o fechas de procesamiento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc03d155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en el dataset: ['SEQN', 'Sex', 'Age', 'Race', 'Education', 'IncomeRatio', 'HeartDisease', 'SystolicBP', 'BMI', 'WaistCircumference', 'Height', 'TotalCholesterol', 'Triglycerides', 'LDL', 'HbA1c', 'Glucose', 'Creatinine', 'UricAcid', 'ALT_Enzyme', 'Albumin', 'Potassium', 'Sodium', 'GGT_Enzyme', 'AST_Enzyme', 'Smoking', 'PhysicalActivity', 'HealthInsurance']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Alcohol'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 3. Filtrar\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnas en el dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m---> 51\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Muestreo (si aplica)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMPLE_FRAC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OMAR\\miniconda3\\envs\\xgb_env\\lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OMAR\\miniconda3\\envs\\xgb_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OMAR\\miniconda3\\envs\\xgb_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Alcohol'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 1. Cargar Configuraci칩n y Datos\n",
    "with open('../models/model_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "df = pd.read_parquet('../data/02_intermediate/process_data.parquet')\n",
    "\n",
    "# --- Mapeo de Traducci칩n ---\n",
    "rename_dict = {\n",
    "    'TARGET': 'HeartDisease',\n",
    "    'Sexo': 'Sex',\n",
    "    'Edad': 'Age',\n",
    "    'Raza': 'Race',\n",
    "    'Educacion': 'Education',\n",
    "    'Ingresos_Ratio': 'IncomeRatio',\n",
    "    'Seguro_Medico': 'HealthInsurance',\n",
    "    'Presion_Sistolica': 'SystolicBP',\n",
    "    'Cintura': 'WaistCircumference',\n",
    "    'BMI': 'BMI',\n",
    "    'Altura': 'Height',\n",
    "    'Colesterol_Total': 'TotalCholesterol',\n",
    "    'Trigliceridos': 'Triglycerides',\n",
    "    'LDL': 'LDL',\n",
    "    'Glucosa': 'Glucose',\n",
    "    'HbA1c': 'HbA1c',\n",
    "    'Creatinina': 'Creatinine',\n",
    "    'Acido_Urico': 'UricAcid',\n",
    "    'Albumina': 'Albumin',\n",
    "    'Enzima_ALT': 'ALT_Enzyme',\n",
    "    'Enzima_AST': 'AST_Enzyme',\n",
    "    'Enzima_GGT': 'GGT_Enzyme',\n",
    "    'Potasio': 'Potassium',\n",
    "    'Sodio': 'Sodium',\n",
    "    'Fumador': 'Smoking',\n",
    "    'Actividad_Fisica': 'PhysicalActivity',\n",
    "    'Alcohol': 'Alcohol'\n",
    "}\n",
    "\n",
    "# Aplicar traducci칩n\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "# 2. Definir Features\n",
    "# --- CORRECCI칍N AQU칈: Usamos 'numeric_features' (como est치 en tu JSON) ---\n",
    "features = config['numeric_features'] + config['categorical_features'] \n",
    "target = config['target']\n",
    "\n",
    "# 3. Filtrar\n",
    "print(\"Columnas en el dataset:\", df.columns.tolist())\n",
    "df = df[features + [target]]\n",
    "\n",
    "# Muestreo (si aplica)\n",
    "if config.get('SAMPLE_FRAC', 1.0) < 1.0:\n",
    "    df = df.sample(frac=config['SAMPLE_FRAC'], random_state=42)\n",
    "\n",
    "print(f\"Dataset cargado y traducido: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (9000, 27)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['SystolicBP', 'TotalCholesterol', 'Triglycerides', 'Glucose', 'UricAcid', 'Creatinine', 'WaistCircumference', 'Height', 'Sex', 'Smoking', 'Alcohol', 'PhysicalActivity', 'HealthInsurance', 'HeartDisease'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m categorical_features \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_features\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Filter only relevant columns\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SAMPLE_FRAC \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m     23\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39mSAMPLE_FRAC, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\OMAR\\miniconda3\\envs\\xgb_env\\lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OMAR\\miniconda3\\envs\\xgb_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OMAR\\miniconda3\\envs\\xgb_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['SystolicBP', 'TotalCholesterol', 'Triglycerides', 'Glucose', 'UricAcid', 'Creatinine', 'WaistCircumference', 'Height', 'Sex', 'Smoking', 'Alcohol', 'PhysicalActivity', 'HealthInsurance', 'HeartDisease'] not in index\""
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. LOAD DATA\n",
    "# ==========================================\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Data file not found at {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "print(f\"Original Data Shape: {df.shape}\")\n",
    "\n",
    "# Load Schema Config\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "features = config['features']\n",
    "target = config['target']\n",
    "numeric_features = config['numeric_features']\n",
    "categorical_features = config['categorical_features']\n",
    "\n",
    "# Filter only relevant columns\n",
    "df = df[features + [target]]\n",
    "\n",
    "if SAMPLE_FRAC < 1.0:\n",
    "    df = df.sample(frac=SAMPLE_FRAC, random_state=42)\n",
    "    print(f\"Sampled Data Shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"Using Full Dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuraci칩n del Experimento (Setup)\n",
    "\n",
    "La funci칩n `setup()` inicializa el entorno de PyCaret y crea el pipeline de transformaci칩n.\n",
    "- **normalize=True**: Escala las variables para que tengan rangos comparables. Usamos `RobustScaler` para ser resilientes a outliers.\n",
    "- **remove_outliers=True**: Elimina anomal칤as estad칤sticas que podr칤an sesgar el modelo.\n",
    "- **fix_imbalance=True**: Aplica SMOTE para generar muestras sint칠ticas de la clase minoritaria (Enfermos), mejorando el aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 游댳 Paso 3: Inicializaci칩n del Experimento (PyCaret Setup)\n",
    "Configuramos el pipeline de preprocesamiento autom치tico con `setup()`. Aqu칤 definimos la \"magia\" de PyCaret:\n",
    "- **Normalizaci칩n**: Aplicamos `RobustScaler` (`normalize_method='robust'`) para escalar los datos manejando bien los outliers t칤picos de datos cl칤nicos.\n",
    "- **Balanceo de Clases**: Activamos `fix_imbalance=True` (SMOTE) para generar datos sint칠ticos de la clase minoritaria (pacientes enfermos), evitando que el modelo se sesgue hacia la clase mayoritaria (sanos).\n",
    "- **Tipos de Datos**: Definimos expl칤citamente cu치les son num칠ricas y cu치les categ칩ricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. SETUP PYCARET\n",
    "# ==========================================\n",
    "# normalize=True (RobustScaler)\n",
    "# remove_outliers=True\n",
    "# fix_imbalance=True\n",
    "\n",
    "exp = setup(\n",
    "    data=df,\n",
    "    target=target,\n",
    "    numeric_features=numeric_features,\n",
    "    categorical_features=categorical_features,\n",
    "    normalize=True,\n",
    "    normalize_method='robust',\n",
    "    remove_outliers=True,\n",
    "    fix_imbalance=True,\n",
    "    session_id=123,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparaci칩n y Selecci칩n de Modelos\n",
    "\n",
    "Entrenamos m칰ltiples algoritmos (Logistic Regression, XGBoost, Random Forest, etc.) con validaci칩n cruzada (Cross-Validation).\n",
    "**M칠trica Clave: Recall**. Buscamos maximizar la capacidad del modelo para detectar casos positivos reales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 游댳 Paso 4: Entrenamiento y Comparaci칩n de Modelos\n",
    "Ejecutamos `compare_models()` para entrenar y evaluar m칰ltiples algoritmos (Random Forest, XGBoost, LightGBM, etc.) simult치neamente.\n",
    "**Criterio de Selecci칩n**: Ordenamos por `Recall` (`sort='Recall'`).\n",
    "**Justificaci칩n Cl칤nica**: En el diagn칩stico de enfermedades, priorizamos minimizar los Falsos Negativos. Preferimos que el modelo tenga \"falsas alarmas\" (baja precisi칩n) a que deje pasar un caso de enfermedad card칤aca real (alto recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e8329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. COMPARE & TRAIN (Modern Tree-Based Only)\n",
    "# ==========================================\n",
    "# Filter: Only Tree-based Ensemble methods\n",
    "# n_select=3: Keep top 3 candidates\n",
    "# sort='Recall': Prioritize Sensitivity (Medical Context)\n",
    "best_models = compare_models(\n",
    "    include=['xgboost', 'lightgbm', 'catboost', 'rf', 'gbc'],\n",
    "    sort='Recall',\n",
    "    n_select=3\n",
    ")\n",
    "print(f\"Top 3 Models Selected: {best_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ebbabd",
   "metadata": {},
   "source": [
    "### 游댳 Paso 4.1: Optimizaci칩n Profunda de Hiperpar치metros\n",
    "Ya tenemos los 3 mejores candidatos ('best_models'). Ahora, no nos conformamos con sus par치metros por defecto. \n",
    "Ejecutamos un proceso de **Tuning Exhaustivo** para cada uno:\n",
    "- **optimize='Recall'**: El algoritmo de b칰squeda intentar치 maximizar espec칤ficamente la sensibilidad.\n",
    "- **n_iter=50**: Probamos 50 combinaciones de hiperpar치metros distintas. 쯇or qu칠 50? En medicina, la diferencia entre un recall del 85% y 87% puede significar salvar m치s vidas (menos Falsos Negativos). Una b칰squeda superficial (n_iter=10) podr칤a perder el 칩ptimo global.\n",
    "- **choose_better=True**: Si despu칠s de tunear el modelo empeora, nos quedamos con la versi칩n original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3.1 DEEP HYPERPARAMETER TUNING\n",
    "# ==========================================\n",
    "tuned_best_models = []\n",
    "\n",
    "print(\"Starting Deep Tuning Session...\")\n",
    "for i, model in enumerate(best_models):\n",
    "    print(f\"\\n--- Tuning Model {i+1}/{len(best_models)}: {type(model).__name__} ---\")\n",
    "    \n",
    "    # Tuning loop\n",
    "    tuned_model = tune_model(\n",
    "        model,\n",
    "        optimize='Recall',\n",
    "        n_iter=50,  # Deep search to minimize False Negatives\n",
    "        choose_better=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Calculate improvement (simple check)\n",
    "    try:\n",
    "        # pull() gets the metrics dataframe from the last run\n",
    "        metrics = pull()\n",
    "        recall_score = metrics.loc['Mean', 'Recall']\n",
    "        print(f\"Model Recall after tuning: {recall_score:.4f}\")\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    tuned_best_models.append(tuned_model)\n",
    "\n",
    "# Select the absolute best from the tuned list manually or by trusting automl\n",
    "# PyCaret's automl() can pick the best from a list based on metric\n",
    "final_best_model = automl(optimize='Recall')\n",
    "print(f\"\\nFINAL BEST MODEL SELECTED: {final_best_model}\")\n",
    "\n",
    "# Explanation print\n",
    "print(\"\\n[INFO] Why n_iter=50? In medical diagnostics, the cost of a False Negative is critical.\")\n",
    "print(\"Increasing search iterations allows the optimizer to explore edge cases in the hyperparameter space\")\n",
    "print(\"that specifically boost Sensitivity (Recall), ensuring fewer sick patients are missed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628056e5",
   "metadata": {},
   "source": [
    "## 5. Finalizaci칩n y Persistencia\n",
    "\n",
    "Una vez seleccionado el mejor modelo:\n",
    "1. **Finalize**: Se re-entrena el modelo utilizando el 100% de los datos (incluyendo el set de prueba reservado anteriormente).\n",
    "2. **Save**: Se guarda el pipeline completo (preprocesamiento + modelo) en un archivo `.pkl` para su despliegue en la API/Streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c900cad",
   "metadata": {},
   "source": [
    "### 游댳 Paso 5: Finalizaci칩n y Serializaci칩n del Modelo\n",
    "Una vez seleccionado el mejor algoritmo:\n",
    "1.  **Finalize**: Re-entrenamos el modelo utilizando **todos** los datos disponibles (incluyendo el set de validaci칩n que PyCaret retuvo internamente).\n",
    "2.  **Save**: Guardamos el pipeline completo como un archivo `.pkl` en el directorio `models/`. Este archivo contiene tanto el modelo predictivo como las transformaciones de datos (escalado, imputaci칩n), listo para ser consumido por la API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. FINALIZE & SAVE\n",
    "# ==========================================\n",
    "final_model = finalize_model(final_best_model)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "save_path = os.path.join(MODEL_DIR, MODEL_NAME)\n",
    "save_model(final_model, save_path)\n",
    "print(f\"Model saved successfully to {save_path}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
