{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 6: Análisis de Interpretabilidad (SHAP) y Ética (Fairness)\n",
    "\n",
    "Este notebook implementa las recomendaciones técnicas para el Sprint 6, enfocándose en:\n",
    "1.  **Interpretabilidad Eficiente:** Uso de SHAP con sampling del dataset de entrenamiento.\n",
    "2.  **Análisis de Ética:** Evaluación de sesgos utilizando `fairlearn` (FNR Parity).\n",
    "\n",
    "**Nota:** Se utiliza una muestra del dataset para demostración, pero el código está preparado para escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pycaret.classification import load_model, predict_model, get_config\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from sklearn.metrics import recall_score, accuracy_score, confusion_matrix\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure project root is in path\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar Modelo y Datos\n",
    "Cargamos el pipeline final y el dataset de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the finalized pipeline\n",
    "try:\n",
    "    pipeline = load_model('../models/best_pipeline')\n",
    "    print(\"Pipeline loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading pipeline: {e}\")\n",
    "    # Fallback for dev environment if model doesn't exist yet\n",
    "    pipeline = None\n",
    "\n",
    "# Load processed data (Simulating fetching X_test)\n",
    "try:\n",
    "    data = pd.read_parquet('../data/02_intermediate/processed_data.parquet')\n",
    "    # Split for demonstration (In real scenario, use the split saved during training)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = train_data.drop(columns=['CVDINFR4'])\n",
    "    y_train = train_data['CVDINFR4']\n",
    "    X_test = test_data.drop(columns=['CVDINFR4'])\n",
    "    y_test = test_data['CVDINFR4']\n",
    "    \n",
    "    print(f\"Data loaded. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    X_train, X_test, y_test = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interpretabilidad con SHAP (Optimizado)\n",
    "Calculamos los valores SHAP utilizando una muestra del set de entrenamiento como background data para eficiencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline and X_train is not None:\n",
    "    # Extract the model from the pipeline (assuming PyCaret structure)\n",
    "    # Note: PyCaret pipeline steps usually include 'trained_model'\n",
    "    try:\n",
    "        model = pipeline.named_steps['trained_model']\n",
    "    except:\n",
    "        # If not in named_steps, might be the last step\n",
    "        model = pipeline[-1]\n",
    "\n",
    "    # Sampling background data (Optimization for Large Datasets)\n",
    "    background_sample = X_train.sample(n=min(100, len(X_train)), random_state=42)\n",
    "    \n",
    "    # Initialize Explainer\n",
    "    # Note: For Tree models use TreeExplainer. For others, KernelExplainer (slower).\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        print(\"SHAP values calculated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"TreeExplainer failed (model might not be tree-based or compatible): {e}\")\n",
    "        print(\"Attempting KernelExplainer (slower)...\")\n",
    "        explainer = shap.KernelExplainer(model.predict, background_sample)\n",
    "        shap_values = explainer.shap_values(X_test.sample(n=min(50, len(X_test)), random_state=42)) # Test on small sample for speed in dev\n",
    "\n",
    "    # Summary Plot\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_test)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis de Casos (Waterfall)\n",
    "Identificamos casos específicos: TP, TN, FP, FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline and X_test is not None:\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    results_df = X_test.copy()\n",
    "    results_df['Actual'] = y_test\n",
    "    results_df['Predicted'] = y_pred\n",
    "    \n",
    "    # Find indices for each case\n",
    "    tp_idx = results_df[(results_df['Actual'] == 1) & (results_df['Predicted'] == 1)].index\n",
    "    tn_idx = results_df[(results_df['Actual'] == 0) & (results_df['Predicted'] == 0)].index\n",
    "    fp_idx = results_df[(results_df['Actual'] == 0) & (results_df['Predicted'] == 1)].index\n",
    "    fn_idx = results_df[(results_df['Actual'] == 1) & (results_df['Predicted'] == 0)].index\n",
    "    \n",
    "    print(f\"Found: {len(tp_idx)} TP, {len(tn_idx)} TN, {len(fp_idx)} FP, {len(fn_idx)} FN\")\n",
    "    \n",
    "    # Function to plot waterfall\n",
    "    def plot_waterfall(index, title):\n",
    "        if len(index) > 0:\n",
    "            idx = index[0]\n",
    "            # Locating the position in X_test to match shap_values index if numpy array\n",
    "            # But shap_values might be list of arrays for classification\n",
    "            # Assuming binary classification, shap_values[1] is for positive class\n",
    "            \n",
    "            # Need numeric index for shap_values\n",
    "            numeric_idx = X_test.index.get_loc(idx)\n",
    "            \n",
    "            print(f\"--- {title} (Index: {idx}) ---\")\n",
    "            # Handle shap_values structure (list for classification vs array for regression)\n",
    "            sv = shap_values[1][numeric_idx] if isinstance(shap_values, list) else shap_values[numeric_idx]\n",
    "            exp_val = explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value\n",
    "            \n",
    "            shap.waterfall_plot(\n",
    "                shap.Explanation(values=sv, \n",
    "                                 base_values=exp_val, \n",
    "                                 data=X_test.iloc[numeric_idx],\n",
    "                                 feature_names=X_test.columns)\n",
    "            )\n",
    "            plt.show()\n",
    "    \n",
    "    # Plotting one of each\n",
    "    plot_waterfall(fn_idx, \"False Negative (Critical)\")\n",
    "    plot_waterfall(fp_idx, \"False Positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ética y Sesgos (Fairlearn)\n",
    "Evaluamos la paridad de Tasa de Falsos Negativos (FNR) en grupos protegidos (Sexo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline and X_test is not None and 'SEXVAR' in X_test.columns: # Assuming SEXVAR is the column name\n",
    "    # Map SEXVAR back to readable if encoded (assuming 1=Male, 2=Female based on BRFSS usually)\n",
    "    # Adjust column name based on actual data schema\n",
    "    sensitive_feature = X_test['SEXVAR'] \n",
    "    \n",
    "    # MetricFrame\n",
    "    # We focus on Recall (Sensitivity). Low recall = High FNR.\n",
    "    metric_frame = MetricFrame(\n",
    "        metrics=recall_score,\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred,\n",
    "        sensitive_features=sensitive_feature\n",
    "    )\n",
    "    \n",
    "    print(\"Recall per group:\")\n",
    "    print(metric_frame.by_group)\n",
    "    \n",
    "    # Plot\n",
    "    metric_frame.by_group.plot(kind='bar', title='Recall by Sex')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.show()\n",
    "    \n",
    "    # FNR Calculation (1 - Recall)\n",
    "    fnr_frame = MetricFrame(\n",
    "        metrics=lambda y_t, y_p: 1 - recall_score(y_t, y_p),\n",
    "        y_true=y_test,\n",
    "        y_pred=y_pred,\n",
    "        sensitive_features=sensitive_feature\n",
    "    )\n",
    "    print(\"\\nFalse Negative Rate (FNR) per group:\")\n",
    "    print(fnr_frame.by_group)\n",
    "else:\n",
    "    print(\"Skipping Fairness analysis: Pipeline, Data or 'SEXVAR' column missing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
