| Título del Concepto | Propiedades Matemáticas Clave | Uso Dentro del Proyecto |
| :--- | :--- | :--- |
| **1. La Función Objetivo General** | $Obj = L(\Theta) + \Omega(\Theta)$ <br> Es la suma de la pérdida total y la regularización total. | Define el objetivo matemático global que el modelo busca minimizar durante el entrenamiento para cumplir con el **Objetivo General**. |
| **2.1. El Término de Pérdida (Loss)** | $L = \sum_{i} l(y_i, \hat{y}_i)$ <br> Mide la diferencia entre las predicciones ($\hat{y}_i$) y los valores reales ($y_i$). | Permite cuantificar el error del modelo. Se relaciona directamente con las métricas de evaluación como Accuracy y AUC-ROC (**Objetivo Específico 4**). |
| **2.2. El Término de Regularización (Regularization)** | $\Omega = \sum_{k} \Omega(f_k)$ <br> Penaliza la complejidad del modelo para evitar el sobreajuste. | Ayuda a crear un modelo más robusto y generalizable, evitando que memorice los datos de entrenamiento (**Objetivo Específico 3 y 4**). |
| **3. El Modelo Aditivo** | $\hat{y}_i^{(t)} = \hat{y}_i^{(t-1)} + f_t(x_i)$ <br> La predicción final es la suma de las predicciones de todos los árboles. | Es la base del boosting. Permite que cada nuevo árbol corrija los errores del anterior, mejorando progresivamente la precisión del ensamble. |
| **4. Aproximación de Taylor** | $f(x+\Delta x) \approx f(x) + f'(x)\Delta x + \frac{1}{2}f''(x)\Delta x^2$ | Es el "truco" matemático que simplifica la función objetivo, permitiendo optimizarla de manera eficiente usando gradientes y hessianos. |
| **4.1. Gradiente ($g_i$) y Hessiano ($h_i$)** | $g_i = \partial_{\hat{y}^{(t-1)}} l(y_i, \hat{y}^{(t-1)})$ <br> $h_i = \partial^2_{\hat{y}^{(t-1)}} l(y_i, \hat{y}^{(t-1)})$ | Son la primera y segunda derivada de la función de pérdida. Guían al modelo sobre *cómo* corregir los errores en cada paso del entrenamiento. |
| **5. Función Objetivo Refinada** | $Obj^{(t)} \approx \sum_{j=1}^{T} [G_j w_j + \frac{1}{2}(H_j + \lambda)w_j^2] + \gamma T$ | Es la ecuación que el algoritmo optimiza en cada paso para construir un nuevo árbol, ya expresada en términos de sumas de gradientes ($G_j$) y hessianos ($H_j$). |
| **6. Peso Óptimo de las Hojas ($w_j^*$ )** | $w_j^* = - \frac{G_j}{H_j + \lambda}$ | Fórmula que calcula el valor de predicción óptimo para cada hoja del árbol, minimizando la función objetivo. |
| **7. Puntuación de Estructura (Score)** | $Obj^* = - \frac{1}{2} \sum_{j=1}^{T} \frac{G_j^2}{H_j + \lambda} + \gamma T$ | Asigna una puntuación de calidad a una estructura de árbol determinada. Un valor más bajo indica un mejor árbol. |
| **8. Métrica de Ganancia (Gain)** | $Gain = \frac{1}{2} [\frac{G_L^2}{H_L+\lambda} + \frac{G_R^2}{H_R+\lambda} - \frac{(G_L+G_R)^2}{H_L+H_R+\lambda}] - \gamma$ | Mide cuánto mejora la "Puntuación de Estructura" al añadir una división. Es el criterio que usa XGBoost para construir el árbol y es fundamental para la interpretabilidad (**Objetivo Específico 5**). |