# Implementaci칩n Te칩rica
| Categor칤a | Recurso Sugerido | Descripci칩n / Por qu칠 leerlo |
| :--- | :--- | :--- |
| 游 **Paper Original** | **XGBoost: A Scalable Tree Boosting System** <br> (*T. Chen, C. Guestrin*) <br> [Enlace](https://arxiv.org/pdf/1603.02754.pdf) | Documento can칩nico. Explica la formulaci칩n matem치tica, el algoritmo de divisi칩n y las optimizaciones del sistema. Esencial para un entendimiento profundo. |
| **Fundamentos** | **A Gentle Introduction to Gradient Boosting** <br> (*Machine Learning Mastery*) <br> [Enlace](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/) | Explica la intuici칩n detr치s del Gradient Boosting de forma accesible, aclarando c칩mo se corrigen los errores de los modelos anteriores. |
| **Fundamentos** | **Gradient Boosting Explained** <br> (*Art칤culo visual interactivo*) <br> [Enlace](http://explained.ai/gradient-boosting/index.html) | Una de las mejores explicaciones visuales. Permite jugar con los par치metros para ver su efecto en tiempo real. |
| **츼rboles de Decisi칩n** | **Decision Trees in Machine Learning** <br> (*Towards Data Science*) <br> [Enlace](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052) | Cubre conceptos clave como la impureza de Gini y la entrop칤a, que son los criterios para realizar las divisiones en las ramas del 치rbol. |
| **Gu칤as Pr치cticas** | **A Guide to Gradient Boosting with XGBoost** <br> (*Tutorial detallado*) <br> [Enlace](https://www.google.com/search?q=https://www.gormanalysis.com/blog/a-guide-to-gradient-boosting-with-xgboost/) | Conecta la teor칤a del gradient boosting con la implementaci칩n espec칤fica de XGBoost, incluyendo ejemplos de c칩digo. Un gran puente entre teor칤a y pr치ctica. |
| **Estado del Arte** | **CatBoost vs. Light GBM vs. XGBoost** <br> (*Art칤culo comparativo*) <br> [Enlace](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db) | Ofrece una perspectiva m치s amplia al comparar XGBoost con sus competidores directos, ayudando a entender sus fortalezas y debilidades relativas. |


# Revisi칩n de Papers Relacionados
| Categor칤a | Recurso Sugerido (Paper) | Descripci칩n / Aporte al Proyecto |
| :--- | :--- | :--- |
| 游뽘 **Aplicaci칩n Directa** | **An Explainable Artificial Intelligence (XAI) Methodology for Heart Disease Classification** <br> (*O. M. Yaseen & M. M. Rashid, 2025*) | **S칤, es un excelente ejemplo de una aplicaci칩n avanzada.** Aplica XGBoost para la clasificaci칩n de enfermedades card칤acas y muestra un pipeline claro. Su principal valor es que integra directamente la capa de explicabilidad (XAI) con **SHAP y LIME**, sirviendo como un modelo a seguir no solo para la predicci칩n, sino tambi칠n para la interpretaci칩n de los resultados
| 游늵 **Comparativa de Modelos** | **Comparative Study of Machine Learning Algorithms in Detecting Cardiovascular Diseases** <br> (*Dayana K et al.*) <br> [Enlace](https://arxiv.org/pdf/2405.17059)|**Perfecto para este apartado.** El objetivo principal de este paper es exactamente lo que buscas: compara directamente el rendimiento de **XGBoost** con otros algoritmos clave (Regresi칩n Log칤stica, Random Forest, etc.) para la detecci칩n de CVD. Te servir치 como una excelente plantilla para tu **Objetivo Espec칤fico 3**, tanto en la metodolog칤a a seguir como en la forma de presentar los resultados. |
| 游 **Inteligencia Artificial Explicable (XAI)** | **Explainable SHAP-XGBoost models for in-hospital mortality...** <br> (*C. Tarabanis et al., 2023*) <br> (*R. A. Carrasco et al.*) <br> [Enlace](https://www.sciencedirect.com/science/article/pii/S2666693623000361?via%3Dihub) | **Paper crucial y de aplicaci칩n directa.** Se enfoca espec칤ficamente en la interpretabilidad de XGBoost en cardiolog칤a usando SHAP. Proporciona ejemplos excelentes y una discusi칩n detallada de los gr치ficos de SHAP (importancia, resumen, dependencia e interacci칩n), sirviendo como una gu칤a perfecta para cumplir el **Objetivo Espec칤fico 5**. |
| 游늳 **Optimizaci칩n** | **Cap칤tulo 29: Boosting y el algoritmo XGBoost** <br> (*R. A. Carrasco et al.*) <br> [Enlace](https://cdr-book.github.io/cap-boosting-xgboost.html)| **Excelente gu칤a metodol칩gica.** Este cap칤tulo de libro es ideal para tu objetivo. Aporta: <br> 1. Una **estrategia paso a paso** para abordar la optimizaci칩n de hiperpar치metros (Secci칩n 29.3.2). <br> 2. Una explicaci칩n clara de los hiperpar치metros de XGBoost, incluyendo los de regularizaci칩n ($\gamma, \alpha, \lambda$). <br> 3. Ejemplos de c칩mo definir una "parrilla de b칰squeda" (`tuneGrid`). <br> **Nota:** Los ejemplos de c칩digo est치n en R, pero los conceptos y la estrategia son 100% aplicables al proceso en Python. |
| 游빏 **Ing. de Caracter칤sticas** | **Optimized Ensemble Learning Approach with Explainable AI...** <br> (*I. D. Mienye & N. Jere, 2024*)  <br> [Enlace](https://pdfs.semanticscholar.org/3747/6c8130d1b63c9b1894870c9243026c98a24e.pdf) | **Muy bueno, enfocado en la interpretabilidad.** Este paper no se centra en la *selecci칩n* de caracter칤sticas antes del modelo, sino en el **an치lisis de importancia** despu칠s. Es una gu칤a pr치ctica excelente para: <br> 1. Aplicar **SHAP** a un modelo XGBoost optimizado. <br> 2. Generar y interpretar los gr치ficos de `summary plot` y `mean absolute values` (ver **Figuras 1-4**). <br> 3. Justificar qu칠 variables son las m치s influyentes para el diagn칩stico. |
| 游빏 **Ing. de Caracter칤sticas** | **Modelo predictivo en el desarrollo de enfermedades cardiovasculares...** <br> (*A. Valderrama Cardenas, 2024*)  <br> [Enlace](https://openaccess.uoc.edu/server/api/core/bitstreams/a5dca908-d32d-4e4c-a677-b143eba20d0d/content) | **Extremadamente relevante.** Es un TFM que detalla un pipeline completo. Te guiar치 en: <br> 1. El proceso de **selecci칩n de variables** desde un dataset grande (BRFSS). <br> 2. La justificaci칩n de usar un conjunto reducido vs. uno completo (11 vs. 29 variables). <br> 3. El an치lisis post-modelo de **importancia de variables** usando SHAP, feature importance y permutation importance (Secci칩n 4.3.1). |
| 游댧 **Benchmark de Vanguardia** | **Cardiovascular disease risk prediction using automated machine learning** <br> (*A. Alaa et al., 2019*) <br> [Enlace](https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0213653&type=printable) | No es una revisi칩n, sino un estudio original en un dataset masivo (UK Biobank). Es una fuente excelente para establecer un **benchmark de rendimiento**, descubrir **predictores no tradicionales** (ej. "ritmo al caminar") y entender conceptos avanzados para tu discusi칩n, como la diferencia entre "ganancia por informaci칩n" y "ganancia por modelado". |